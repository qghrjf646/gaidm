\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage[margin=2.5cm]{geometry}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{listings}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\ELBO}{\mathcal{L}}
\newcommand{\Normal}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}

\title{Variational Autoencoders: Theory, Implementation, and Conditional Extensions on MNIST}
\author{Alexis Le Trung, Khatir Youyou, Tidjani Adam Kandine, \\ Yahya Ahachim, Aniss Outaleb, Quentin Wurtlin \\ \textit{Course: Generative AI and Diffusion Models}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report presents a comprehensive study of Variational Autoencoders (VAEs), encompassing both theoretical foundations and practical implementation. We derive the Evidence Lower Bound (ELBO) from first principles, implement a convolutional VAE architecture for the MNIST dataset, and investigate training dynamics through KL divergence annealing. We extend the standard VAE to a Conditional VAE (CVAE) that enables controlled generation by conditioning on class labels. Our experiments demonstrate that CVAEs achieve superior reconstruction quality (124.36 vs 138.00) while maintaining lower KL divergence (4.80 vs 6.59), confirming the benefits of incorporating label information. We provide extensive visualizations including latent space embeddings, latent traversals, and interpolations to illustrate the learned representations.
\end{abstract}

\tableofcontents
\newpage

%==============================================================================
\section{Introduction}
%==============================================================================

Generative modeling represents one of the fundamental challenges in machine learning: learning to model the underlying probability distribution of data such that we can generate new, plausible samples. Among the various approaches to generative modeling, Variational Autoencoders (VAEs) \citep{kingma2014auto,rezende2014stochastic} stand out for their elegant combination of deep learning with principled probabilistic inference.

The core idea behind VAEs is to learn a low-dimensional latent representation of data that captures its essential structure. Unlike traditional autoencoders that learn deterministic mappings, VAEs learn probabilistic encodings, enabling them to generate diverse samples by sampling from the learned latent distribution. This probabilistic formulation is grounded in variational Bayesian inference, providing a theoretically principled framework for balancing reconstruction fidelity against regularization of the latent space.

\subsection{Objectives}

This project aims to:
\begin{enumerate}
    \item \textbf{Derive the ELBO}: Present a complete mathematical derivation of the Evidence Lower Bound from first principles, establishing the theoretical foundation for VAE training.
    \item \textbf{Implement a Convolutional VAE}: Build and train a convolutional VAE architecture on the MNIST handwritten digit dataset.
    \item \textbf{Investigate Training Dynamics}: Track reconstruction and KL divergence losses separately, implementing KL annealing to prevent posterior collapse.
    \item \textbf{Visualize Latent Representations}: Produce latent space visualizations, latent traversals, and interpolations to understand the learned representations.
    \item \textbf{Extend to Conditional VAE}: Implement a CVAE that conditions generation on class labels, enabling controlled digit generation.
\end{enumerate}

\subsection{Organization}

Section~\ref{sec:theory} presents the theoretical foundations, including the complete ELBO derivation. Section~\ref{sec:architecture} describes the model architectures for both VAE and CVAE. Section~\ref{sec:training} details the training methodology including KL annealing. Section~\ref{sec:experiments} presents experimental results with extensive visualizations. Section~\ref{sec:discussion} discusses findings and limitations. Section~\ref{sec:conclusion} concludes with future directions.

%==============================================================================
\section{Theoretical Foundations}
\label{sec:theory}
%==============================================================================

\subsection{Problem Setup}

Consider a dataset $\mathcal{D} = \{x^{(1)}, \ldots, x^{(N)}\}$ of i.i.d. samples from an unknown data distribution $p_{\text{data}}(x)$. We assume each observation $x$ is generated by a latent variable $z \in \R^d$ according to the generative process:
\begin{align}
    z &\sim p(z) = \Normal(0, I) \\
    x &\sim p_\theta(x|z)
\end{align}
where $p_\theta(x|z)$ is the decoder network parameterized by $\theta$.

Our goal is to maximize the marginal log-likelihood of the data:
\begin{equation}
    \log p_\theta(x) = \log \int p_\theta(x|z) p(z) \, dz
\end{equation}

This integral is intractable for complex decoder networks, motivating the variational approach.

\subsection{Derivation of the Evidence Lower Bound (ELBO)}

\begin{theorem}[Evidence Lower Bound]
For any distribution $q_\phi(z|x)$ over latent variables, the log marginal likelihood satisfies:
\begin{equation}
    \log p_\theta(x) \geq \E_{q_\phi(z|x)}[\log p_\theta(x|z)] - \KL(q_\phi(z|x) \| p(z)) = \ELBO(\theta, \phi; x)
\end{equation}
\end{theorem}

\begin{proof}
We begin by introducing an arbitrary distribution $q_\phi(z|x)$ (the encoder) and applying Jensen's inequality:

\textbf{Step 1: Importance sampling formulation}
\begin{align}
    \log p_\theta(x) &= \log \int p_\theta(x|z) p(z) \, dz \\
    &= \log \int q_\phi(z|x) \frac{p_\theta(x|z) p(z)}{q_\phi(z|x)} \, dz \\
    &= \log \E_{q_\phi(z|x)}\left[\frac{p_\theta(x|z) p(z)}{q_\phi(z|x)}\right]
\end{align}

\textbf{Step 2: Apply Jensen's inequality}

Since $\log$ is concave, Jensen's inequality gives:
\begin{align}
    \log p_\theta(x) &\geq \E_{q_\phi(z|x)}\left[\log \frac{p_\theta(x|z) p(z)}{q_\phi(z|x)}\right] \\
    &= \E_{q_\phi(z|x)}[\log p_\theta(x|z)] + \E_{q_\phi(z|x)}\left[\log \frac{p(z)}{q_\phi(z|x)}\right] \\
    &= \E_{q_\phi(z|x)}[\log p_\theta(x|z)] - \KL(q_\phi(z|x) \| p(z))
\end{align}

This completes the derivation of the ELBO.
\end{proof}

\subsection{Alternative Derivation via KL Decomposition}

An illuminating alternative derivation reveals the gap between the ELBO and the true log-likelihood:

\begin{align}
    \log p_\theta(x) &= \log p_\theta(x) \int q_\phi(z|x) \, dz \\
    &= \int q_\phi(z|x) \log p_\theta(x) \, dz \\
    &= \E_{q_\phi(z|x)}[\log p_\theta(x)]
\end{align}

Using Bayes' rule $p_\theta(z|x) = p_\theta(x|z)p(z)/p_\theta(x)$:
\begin{align}
    \E_{q_\phi(z|x)}[\log p_\theta(x)] &= \E_{q_\phi(z|x)}\left[\log \frac{p_\theta(x|z)p(z)}{p_\theta(z|x)}\right] \\
    &= \E_{q_\phi(z|x)}[\log p_\theta(x|z)] - \E_{q_\phi(z|x)}\left[\log \frac{q_\phi(z|x)}{p(z)}\right] + \E_{q_\phi(z|x)}\left[\log \frac{q_\phi(z|x)}{p_\theta(z|x)}\right] \\
    &= \underbrace{\E_{q_\phi(z|x)}[\log p_\theta(x|z)] - \KL(q_\phi(z|x) \| p(z))}_{\text{ELBO}} + \underbrace{\KL(q_\phi(z|x) \| p_\theta(z|x))}_{\geq 0}
\end{align}

This shows that the gap equals $\KL(q_\phi(z|x) \| p_\theta(z|x))$, measuring how well the encoder approximates the true posterior.

\subsection{Closed-Form KL Divergence for Gaussians}

We parameterize the encoder as $q_\phi(z|x) = \Normal(\mu_\phi(x), \text{diag}(\sigma^2_\phi(x)))$. With standard Gaussian prior $p(z) = \Normal(0, I)$, the KL divergence has a closed form:

\begin{theorem}[Gaussian KL Divergence]
For $q = \Normal(\mu, \text{diag}(\sigma^2))$ and $p = \Normal(0, I)$ in $d$ dimensions:
\begin{equation}
    \KL(q \| p) = \frac{1}{2} \sum_{j=1}^{d} \left(\mu_j^2 + \sigma_j^2 - \log \sigma_j^2 - 1\right)
\end{equation}
\end{theorem}

\begin{proof}
By definition:
\begin{align}
    \KL(q \| p) &= \E_q[\log q(z)] - \E_q[\log p(z)] \\
    &= -\frac{1}{2}\sum_j \left(1 + \log \sigma_j^2\right) + \frac{1}{2}\E_q\left[\sum_j z_j^2\right] \\
    &= -\frac{1}{2}\sum_j \left(1 + \log \sigma_j^2\right) + \frac{1}{2}\sum_j (\mu_j^2 + \sigma_j^2) \\
    &= \frac{1}{2} \sum_j \left(\mu_j^2 + \sigma_j^2 - \log \sigma_j^2 - 1\right)
\end{align}
\end{proof}

\subsection{Gradients for Optimization}

For gradient-based optimization, we need derivatives of the KL divergence:

\begin{align}
    \frac{\partial D_{KL}}{\partial \mu_j} &= \mu_j \\
    \frac{\partial D_{KL}}{\partial \sigma_j} &= \sigma_j - \frac{1}{\sigma_j} = \frac{\sigma_j^2 - 1}{\sigma_j}
\end{align}

These gradients reveal important dynamics: the KL term penalizes large mean values (pulling $\mu \to 0$) and deviations from unit variance (pulling $\sigma \to 1$).

\subsection{The Reparameterization Trick}

A key innovation of VAEs is the reparameterization trick \citep{kingma2014auto}, which enables backpropagation through stochastic sampling.

\begin{definition}[Reparameterization Trick]
Instead of sampling $z \sim \Normal(\mu, \sigma^2)$ directly, we write:
\begin{equation}
    z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \Normal(0, I)
\end{equation}
\end{definition}

This transforms a stochastic node into a deterministic function of parameters plus independent noise, enabling gradient flow:
\begin{align}
    \frac{\partial z}{\partial \mu} &= I \\
    \frac{\partial z}{\partial \sigma} &= \text{diag}(\epsilon)
\end{align}

The expectation $\E_{q_\phi(z|x)}[f(z)]$ becomes $\E_{\epsilon \sim \Normal(0,I)}[f(\mu + \sigma \odot \epsilon)]$, and we can use Monte Carlo estimation with a single sample during training.

%==============================================================================
\section{Model Architecture}
\label{sec:architecture}
%==============================================================================

\subsection{Convolutional VAE Architecture}

We implement a convolutional VAE suitable for the 28×28 grayscale MNIST images. The architecture follows an encoder-decoder structure with a 2-dimensional latent space for visualization purposes.

\subsubsection{Encoder Network}

The encoder maps input images to latent distribution parameters:

\begin{table}[h]
\centering
\caption{Encoder Architecture}
\begin{tabular}{lccc}
\toprule
Layer & Output Shape & Kernel & Stride \\
\midrule
Input & $1 \times 28 \times 28$ & -- & -- \\
Conv2d + ReLU & $32 \times 14 \times 14$ & $4 \times 4$ & 2 \\
Conv2d + ReLU & $64 \times 7 \times 7$ & $4 \times 4$ & 2 \\
Conv2d + ReLU & $128 \times 4 \times 4$ & $3 \times 3$ & 2 \\
Flatten & $2048$ & -- & -- \\
Linear $\to \mu$ & $2$ & -- & -- \\
Linear $\to \log\sigma^2$ & $2$ & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Decoder Network}

The decoder maps latent vectors back to image space:

\begin{table}[h]
\centering
\caption{Decoder Architecture}
\begin{tabular}{lccc}
\toprule
Layer & Output Shape & Kernel & Stride \\
\midrule
Input & $2$ & -- & -- \\
Linear + ReLU & $2048$ & -- & -- \\
Reshape & $128 \times 4 \times 4$ & -- & -- \\
ConvTranspose2d + ReLU & $64 \times 7 \times 7$ & $3 \times 3$ & 2 \\
ConvTranspose2d + ReLU & $32 \times 14 \times 14$ & $4 \times 4$ & 2 \\
ConvTranspose2d + Sigmoid & $1 \times 28 \times 28$ & $4 \times 4$ & 2 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Conditional VAE Architecture}

The CVAE extends the VAE by conditioning both encoder and decoder on class labels $c \in \{0, 1, \ldots, 9\}$.

\subsubsection{Label Conditioning Mechanism}

We incorporate labels through spatial broadcasting:
\begin{enumerate}
    \item Convert label $c$ to one-hot vector $\mathbf{c} \in \R^{10}$
    \item Broadcast to spatial dimensions: $\mathbf{C} \in \R^{10 \times H \times W}$
    \item Concatenate with input: encoder receives $(1+10) \times 28 \times 28$
    \item Decoder also receives label embedding concatenated with $z$
\end{enumerate}

The modified generative model becomes:
\begin{align}
    z &\sim p(z) = \Normal(0, I) \\
    x &\sim p_\theta(x|z, c)
\end{align}

And the CVAE objective:
\begin{equation}
    \ELBO_{\text{CVAE}} = \E_{q_\phi(z|x,c)}[\log p_\theta(x|z,c)] - \KL(q_\phi(z|x,c) \| p(z))
\end{equation}

Note that we use a class-independent prior $p(z) = \Normal(0, I)$, which encourages the latent space to encode style variations independent of class identity.

%==============================================================================
\section{Training Methodology}
\label{sec:training}
%==============================================================================

\subsection{Loss Function}

The VAE loss function combines reconstruction and regularization terms:
\begin{equation}
    \mathcal{L}(\theta, \phi; x) = -\ELBO = \underbrace{\text{BCE}(x, \hat{x})}_{\text{Reconstruction}} + \underbrace{\beta \cdot \KL(q_\phi(z|x) \| p(z))}_{\text{Regularization}}
\end{equation}

where BCE denotes binary cross-entropy (appropriate for normalized pixel values), and $\beta$ is a weighting factor.

\subsection{KL Annealing}

A common challenge in VAE training is \textbf{posterior collapse} \citep{bowman2016generating}, where the model ignores the latent variable and the encoder produces $q_\phi(z|x) \approx p(z)$ regardless of $x$.

To address this, we implement KL annealing (also called $\beta$-scheduling):
\begin{equation}
    \beta(t) = \min\left(1, \frac{t}{T_{\text{warmup}}}\right)
\end{equation}

where $t$ is the current epoch and $T_{\text{warmup}}$ is the warmup period. This allows the model to first learn good reconstructions before gradually enforcing the prior constraint.

\subsection{Training Configuration}

\begin{table}[h]
\centering
\caption{Training Hyperparameters}
\begin{tabular}{lcc}
\toprule
Parameter & VAE & CVAE \\
\midrule
Latent dimension & 2 & 2 \\
Batch size & 128 & 128 \\
Learning rate & $10^{-3}$ & $10^{-3}$ \\
Optimizer & Adam & Adam \\
Epochs & 30 & 30 \\
KL warmup epochs & 10 & 10 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Monitoring Training}

We track three metrics during training:
\begin{enumerate}
    \item \textbf{Reconstruction Loss}: BCE between input and reconstruction
    \item \textbf{KL Divergence}: Regularization term (before $\beta$ weighting)
    \item \textbf{Reconstruction/KL Ratio}: Indicator of balance; values 10-50 suggest healthy training
\end{enumerate}

%==============================================================================
\section{Experiments and Results}
\label{sec:experiments}
%==============================================================================

\subsection{Dataset}

We use the MNIST dataset \citep{lecun1998mnist} consisting of 60,000 training and 10,000 test images of handwritten digits (0-9). Images are 28×28 grayscale, normalized to $[0, 1]$.

\subsection{VAE Training Results}

\subsubsection{Training Dynamics}

Figure~\ref{fig:vae_training} shows the training curves for the standard VAE. Key observations:

\begin{itemize}
    \item \textbf{Reconstruction loss} decreases steadily from ~160 to ~138, indicating improving reconstruction quality.
    \item \textbf{KL divergence} shows the characteristic ``bump'' during warmup (epochs 1-10) as $\beta$ increases, then stabilizes around 6.6.
    \item \textbf{Recon/KL ratio} of ~20 indicates balanced training without posterior collapse.
\end{itemize}


\subsubsection{Reconstruction Quality}

Visual inspection of reconstructions shows that the VAE captures the essential structure of digits while smoothing fine details—a characteristic of VAE reconstructions due to the variational regularization.

\subsubsection{Latent Space Visualization}

With a 2D latent space, we can directly visualize the learned representation (Figure~\ref{fig:latent_space}). The latent space shows clear clustering by digit class, with semantically similar digits (e.g., 4 and 9, 3 and 8) positioned nearby. This organization emerges purely from the reconstruction objective without any label supervision.


\subsubsection{Latent Traversals}

By fixing one latent dimension and varying the other across $[-3, 3]$, we observe smooth morphological changes (Figure~\ref{fig:traversal}):
\begin{itemize}
    \item $z_1$ (horizontal) primarily controls digit slant and rotation
    \item $z_2$ (vertical) influences stroke thickness and overall scale
\end{itemize}


\subsubsection{Latent Interpolations}

Linear interpolation between encoded digits produces semantically meaningful transitions (Figure~\ref{fig:interpolation}). For example, interpolating from a ``3'' to an ``8'' shows intermediate forms that resemble plausible hybrid digits.


\subsection{CVAE Training Results}

\subsubsection{Training Dynamics}

The CVAE achieves notably better metrics:
\begin{itemize}
    \item \textbf{Final reconstruction loss}: 124.36 (vs 138.00 for VAE)
    \item \textbf{Final KL divergence}: 4.80 (vs 6.59 for VAE)
    \item \textbf{Final total loss}: 129.16 (vs 144.59 for VAE)
\end{itemize}

The improvement is expected: by providing class labels, the decoder doesn't need to infer digit identity from $z$, allowing the latent space to focus purely on within-class variations.

\subsubsection{Conditional Generation}

The CVAE enables controlled generation: given any latent vector $z$ and class label $c$, we can generate a digit of class $c$ with style determined by $z$. Figure~\ref{fig:cvae_generation} shows 10 samples for each digit class, demonstrating consistent class identity with natural style variation.

\subsubsection{Latent Space Structure}

Unlike the VAE's clustered latent space, the CVAE's latent space shows \textbf{mixed class distributions}(Figure~\ref{fig:cve_latent_space}). This is the desired behavior with a class-independent prior: the latent space encodes class-agnostic features (style, stroke characteristics) while class information is provided separately.


\subsubsection{Disentanglement Demonstration}

A key advantage of CVAEs is explicit disentanglement of class identity from other factors of variation. We demonstrate this in Figure~\ref{fig:cvae_disentangle} with three experiments:

\begin{enumerate}
    \item \textbf{Fixed $z$, varying class}: The same latent vector generates different digits when conditioned on different labels, with consistent style across classes.
    
    \item \textbf{Style transfer}: A latent vector extracted from one digit's style can be applied to generate any other digit class with that style.
    
    \item \textbf{Class-conditional traversals}: Latent traversals for a fixed class show morphological variations (slant, thickness) while maintaining class identity perfectly.
\end{enumerate}


\subsection{Quantitative Comparison}

\begin{table}[h]
\centering
\caption{Final Training Metrics Comparison}
\label{tab:comparison}
\begin{tabular}{lccc}
\toprule
Model & Recon. Loss & KL Divergence & Total Loss \\
\midrule
VAE & 138.00 & 6.59 & 144.59 \\
CVAE & 124.36 & 4.80 & 129.16 \\
\midrule
Improvement & 9.9\% & 27.2\% & 10.7\% \\
\bottomrule
\end{tabular}
\end{table}

The CVAE shows consistent improvements across all metrics. The larger relative improvement in KL divergence (27.2\%) suggests that conditioning allows the encoder to learn a more compact representation.

%==============================================================================
\section{Discussion}
\label{sec:discussion}
%==============================================================================

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{KL annealing is essential}: Without gradual warmup, early training showed signs of posterior collapse with KL divergence dropping to near zero.
    
    \item \textbf{2D latent space enables visualization but limits capacity}: While useful for visualization, a 2D latent space constrains the model's representational power. Production VAEs typically use 64-256 dimensions.
    
    \item \textbf{CVAE achieves superior reconstruction through task decomposition}: By separating ``what digit'' (label) from ``how it looks'' (latent), the CVAE can allocate all latent capacity to style variations.
    
    \item \textbf{Latent space structure reflects conditioning}: VAE latent spaces naturally cluster by class; CVAE latent spaces mix classes because labels provide class information externally.
\end{enumerate}

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Reconstruction blurriness}: VAE reconstructions are characteristically blurry compared to input images, a known limitation of pixel-wise reconstruction losses.
    
    \item \textbf{Limited latent dimensions}: Our 2D space cannot capture all factors of variation in handwritten digits.
    
    \item \textbf{Binary assumption}: Using BCE loss assumes binary pixel values, whereas real-valued assumptions (Gaussian decoder) might be more appropriate.
    
    \item \textbf{Architectural choices}: We did not extensively tune architecture or hyperparameters; better results are achievable with deeper networks or different configurations.
\end{enumerate}

\subsection{Relation to Other Work}

Our implementation follows the original VAE formulation \citep{kingma2014auto}. The CVAE extension follows \citet{sohn2015learning}. KL annealing was proposed by \citet{bowman2016generating} for sequence VAEs but applies broadly.

Recent advances include:
\begin{itemize}
    \item \textbf{$\beta$-VAE} \citep{higgins2017beta}: Uses $\beta > 1$ for better disentanglement
    \item \textbf{VQ-VAE} \citep{van2017neural}: Discrete latent spaces
    \item \textbf{Hierarchical VAEs} \citep{vahdat2020nvae}: Multi-scale latent hierarchies
\end{itemize}

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

This project presented a comprehensive study of Variational Autoencoders, from theoretical derivation to practical implementation. We derived the ELBO from first principles, implemented convolutional architectures for both VAE and CVAE, and demonstrated the importance of KL annealing for stable training.

Our experiments on MNIST showed that:
\begin{enumerate}
    \item Standard VAEs learn semantically meaningful latent spaces that cluster by digit class
    \item Latent traversals reveal interpretable factors of variation
    \item CVAEs achieve better reconstruction (9.9\% improvement) by decomposing class identity from style
    \item CVAEs enable explicit control over generated digit class while preserving style variations
\end{enumerate}

\subsection{Future Directions}

Several extensions could improve upon this work:

\begin{enumerate}
    \item \textbf{Higher-dimensional latent spaces}: Increase latent dimension and use t-SNE/UMAP for visualization
    \item \textbf{$\beta$-VAE exploration}: Systematically vary $\beta$ to study disentanglement-reconstruction tradeoff
    \item \textbf{Alternative architectures}: Explore ResNet encoders/decoders or attention mechanisms
    \item \textbf{Different datasets}: Apply to more complex datasets (CIFAR-10, CelebA)
    \item \textbf{Quantitative disentanglement metrics}: Implement metrics like DCI or $\beta$-VAE metric
\end{enumerate}

%==============================================================================
\section*{Acknowledgments}
%==============================================================================

This work was completed as part of the Generative AI and Diffusion Models course. We thank the course instructor Dr Eric Moulines for his classes and the PyTorch team for their excellent deep learning framework.

%==============================================================================
\appendix
%==============================================================================
\bibliographystyle{plainnat}
\begin{thebibliography}{10}

\bibitem[Bowman et~al.(2016)]{bowman2016generating}
Samuel~R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew~M. Dai, Rafal Józefowicz, and Samy Bengio.
\newblock Generating sentences from a continuous space.
\newblock In \emph{Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)}, pages 10--21, 2016.

\bibitem[Higgins et~al.(2017)]{higgins2017beta}
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner.
\newblock $\beta$-{VAE}: Learning basic visual concepts with a constrained variational framework.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Kingma and Welling(2014)]{kingma2014auto}
Diederik~P. Kingma and Max Welling.
\newblock Auto-encoding variational {B}ayes.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[LeCun et~al.(1998)]{lecun1998mnist}
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem[Rezende et~al.(2014)]{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep generative models.
\newblock In \emph{International Conference on Machine Learning}, pages 1278--1286, 2014.

\bibitem[Sohn et~al.(2015)]{sohn2015learning}
Kihyuk Sohn, Honglak Lee, and Xinchen Yan.
\newblock Learning structured output representation using deep conditional generative models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 3483--3491, 2015.

\bibitem[Vahdat and Kautz(2020)]{vahdat2020nvae}
Arash Vahdat and Jan Kautz.
\newblock {NVAE}: A deep hierarchical variational autoencoder.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[van~den Oord et~al.(2017)]{van2017neural}
Aaron van~den Oord, Oriol Vinyals, and Koray Kavukcuoglu.
\newblock Neural discrete representation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\end{thebibliography}

\section{Collected Figures}
\label{sec:figures}

The following figures are collected here for convenience. Refer to the main text for discussion and figure references.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{vae_train_curves.png}
    \caption{VAE training curves showing reconstruction loss (left), KL divergence (center), and their ratio (right) over 30 epochs. The KL annealing warmup (epochs 1-10) is visible in the KL curve.}
    \label{fig:vae_training}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{vae_latent_space_visualization.png}
    \caption{Visualization of the 2D latent space for 5000 MNIST test samples, colored by digit class. Note the natural clustering by class without any label supervision.}
    \label{fig:latent_space}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{vae_latent_traversal_grid.png}
    \caption{Latent space traversal grid. Each row corresponds to a fixed $z_2$ value, each column to a fixed $z_1$ value, ranging from -3 to +3.}
    \label{fig:traversal}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{vae_latent_interpolations.png}
    \caption{Linear interpolations in latent space between pairs of encoded digits. Each row shows 10 steps from the source digit (left) to the target digit (right).}
    \label{fig:interpolation}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{cvae_latent_space_visualization.png}
    \caption{Visualization of the 2D latent space for 5000 MNIST test samples, colored by digit class. Note the unclear clustering given the label supervision.}
    \label{fig:cve_latent_space}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{cvae_disentangling.png}
    \caption{CVAE disentanglement demonstration. Top rows: same latent interpolation with different class labels (1 vs 7). Bottom row: fixed latent vector $z$ applied to all digit classes 0-9, showing style transfer.}
    \label{fig:cvae_disentangle}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{cvae_conditional_generation.png}
    \caption{CVAE conditional generation: 10 samples for each digit.}
    \label{fig:cvae_generation}
\end{figure}



\end{document}
